\chapter{Introduction}

Much of the history of programming languages has been about increasing abstraction
and generalization. Today we take it for granted that a single suitably powerful
language could be used for nearly any programming task, were it not for pragmatic
factors beyond our control. Over time, many special-purpose languages
have been subsumed by these more powerful and general languages.
The field of technical computing --- programming for applied math and the
sciences --- is something of a holdout from this trend, with its own
purpose-built languages still dominating.
There are probably many reasons for this, including historical, social and
technical.

In this work we will focus on the technical reasons for this, and the
problems that the current state of affairs causes.

What do we mean by a \emph{language} for scientific computing?
The prevailing
answer is that it is about numeric arrays, perhaps matrices specifically,
and numerical libraries that use them.
Instead we intend to argue that, at a
deeper level, it is about certain kinds of flexibility, particularly
flexibility in the behavior of key operators and functions. This flexibility
is needed to maximize composability and reusability in scientific code.
Without it, certain programs may be easy to write today, but changing
functional and performance requirements can become difficult to meet.

It is clear that scientific and technical users have their own particular
needs. So far there have been two paths available for fulfilling these
needs: using a mainstream general-purpose language like C++ or Java, or
designing a new language like R or MATLAB. Designing your own language
seems like a drastic thing to do, and yet it is unusually common in
technical computing (see gang of 40).


\section{The technical computing problem}

\begin{figure}
  \label{PLpriorities}
  \begin{center}
    \begin{tabular}{|l|l|}\hline
      \textbf{Mainstream PL} & \textbf{Technical computing} \\
      \hline \hline
      classes, single dispatch             &  complex operators \\
      \hline
      separate compilation                 &  performance, inlining \\
      \hline
      parametric polymorphism              &  ad-hoc polymorphism, extensibility \\
      % - concrete reuse of generated code
      \hline
      static checking                      &  experimental computing \\
      \hline
      modularity, encapsulation            &  large vocabularies \\
      \hline
      eliminating tags                     &  self-describing data, acceptance of tags \\
      \hline
      data hiding                          &  explicit memory layout \\
      \hline
    \end{tabular}
  \end{center}
  \caption{
    Priorities of mainstream object-oriented and functional programming language research and
    implementation compared to those of the technical computing domain.
  }
\end{figure}

Figure~\ref{PLpriorities} compares the general design priorities of mainstream programming
languages to those of technical computing languages. The priorities in each row are not
necessarily opposites or even mutually exclusive, but rather are a matter of emphasis.
For example, it is certainly possible to have both parametric and ad-hoc polymorphism within
the same language, but syntax, recommended idioms, and the design of the standard library will
tend to emphasize one or the other.

It is striking how different these priorities are. We believe these technical factors have
contributed significantly to the persistence of specialized environments in this area.


\section{Proposed solution}

It is clear that any future scientific computing language will need to be able to
match the performance of C, C++, and Fortran. To do that, it is almost certain
that speculative optimizations such as tracing (TODO cite) will not be sufficient ---
the language will need to be able to \emph{prove} facts about types, or at least
let the user request specific types. It is also clear that such a language must
strive for maximum convenience, or else the split between performance languages
and productivity languages will persist.

It is fair to say that two approaches to this problem are being tried: one is
to design better statically-typed languages, and the other is to apply
program analysis techniques to dynamically-typed languages.
Static type systems seem to be getting close to achieving the desired level
of flexibility (as in Fortress \cite{fortresspec}, for instance), but it
is still too early to call a winner between these two approaches
(if, indeed, there even needs to be a winner).

Our contribution derives from observations of the second approach.
Efforts to analyze and optimize dynamically-typed programs generally make two
assumptions: (1) we should work on analyzing \emph{existing} popular
languages, and (2) users of these languages don't want to use types.
The first assumption makes practical sense.
Convenience is hard to quantify, so using existing languages that have already
been deemed convenient by popular opinion puts us on solid footing.
This thesis describes a new language, so we simply take the negation of the
first assumption as a premise.
Our work addresses the second assumption more directly.
Specifically, we point out that types do not have to be used for
static checking, and that using them for \emph{code selection} and
\emph{code specialization} is particularly useful in technical computing.
This perspective has not been explored thoroughly in the past.



what to dispatch on? dispatch power has been extended in many ways, but
there is no real limit to what somebody might want to dispatch on.
so what to do?
some sets of values are more robust under computation than others
(closure properties).
identify those sets using dataflow concerns.

say we have a method defined for integers, and also for the special cases
``2'' and ``odd integers''. a realistic implementation
will group all of these under ``integer'', and ideally generate a couple
branches to handle the other cases. we argue the concept of ``integer''
here is a more robust set, and so a more fundamental language concept.


somewhat counter-intuitively, dynamic dispatch can be good for performance
since it permits invoking the most specialized possible method.
static overloading can lead to calling a sub-optimal case when multiple
overloads exist for the sake of performance.


(one way to describe it: you can start with the notation you want, or with the
notation that people are using already, and come up with better ways to
*explain* it.
in other words, design a robust framework that assigns a satisfactory meaning
to each thing we'd like to write.
the amazing thing about programming languages it that a better
explanation can directly lead to better performance!)



- mention dual traps of wasted power and divergence


- vs predicate dispatch: we extend dispatch power in a different direction,
guided by semantic subtyping. goal is maximum power that still yields high
likelihood of resolving many calls to a single implementation.

a multimethod system designed for type inference and specialization.

past (static) type systems for dispatch were designed to ensure the absence of
no-method errors and ambiguities (completeness and uniqueness). our goal
is instead to statically resolve methods. this is inherently heuristic and
best-effort. since static types shouldn't affect program behavior, we
conclude that the dispatch must be dynamic, which is happily the same
conclusion you would reach if you simply wanted dynamic typing.


it is quite possible that some static type system will work well for this
however we defer this question.

interesting variants:
- require static single method matches
- reject programs with no-method errors
- reject programs that yield Unions


%problem of finding connections between array programming and OOP.
%array programming is a powerful paradigm for describing computational
%kernels operating over potentially large amounts of data.

%an ``object system'' in this context is often considered a separate
%part of the language, to be used only when arrays no longer
%suffice.

%technical computing systems have an unusually large amount of
%``failure of abstraction'' --- manual duplication of facts
%all over the system. imagine changing from column-major to
%row-major.

%it may be true, as a practical matter, that we as a species need to agree on
%one set of rules and stick to it. however this shouldn't be forced by
%technical limitations.


Solution: integrate code specialization and code selection

Why integrate code specialization and code selection?

\begin{itemize}
\item specialization requires selection anyway
\item simpler system
\item can sometimes collapse 2 layers of dispatch into 1
\item can replace library code with a code generator without either changing
  client code *or* any extra overhead
\end{itemize}

Extensive code specialization is a key feature of
technical computing. ``what to specialize on'' has been an open problem.
our types are a possible solution to this for two reasons:

\begin{enumerate}
\item you can tune the amount of information they contain
\item everybody agrees to use them, which helps ensure that specializing on
  the types will actually do something useful.
\end{enumerate}


The key ingredients:

\begin{singlespace}
\begin{enumerate}
\item Self-describing data model aware of memory layout
\item Type tags with nested structure
\item A fully-connected type tree
\item Dynamic multiple dispatch over all types % including parameters and varargs
\item Dataflow type inference
\item Automatic code specialization
\end{enumerate}
\end{singlespace}

This list of features may appear somewhat ad-hoc. However, they turn out to be
remarkably strongly coupled, and deeply constrained by our ultimate goal.
Each of these features has appeared in some form before, but never in a way
that fully solves the problems described here.

Challenges of this approach (why has this not been done before?)


\section{Contributions}

1 - an analysis of the nature of technical computing that suggests what
sort of language would form a good base for it. it should emphasize
complex operators and code generation/specialization.

2 - the idea of integrating specialization and selection, using multiple dispatch
and semantic subtyping.

3 - an explication through elimination of technical computing language features

``One of the most fruitful techniques of language analysis is explication through
elimination. The basic idea is that one explains a linguistic feature by showing
how one could do without it.'' \cite{morris}

A main contribution of this thesis is the application of this approach to features
of technical computing environments that have not been subject to such analysis
before.
