\chapter{The Julia approach}

\section{Core calculus and data model}

Julia is based on an untyped lambda calculus augmented with generic functions,
tagged data values, mutable cells, and assignment.

\vspace{-3ex}
\begin{singlespace}
\begin{align*}
  e\ ::=\ &\ x                 & \textrm{(variable)} \\
        &\ |\ 0\ |\ 1\ |\ \cdots     & \textrm{(constant)} \\
        &\ |\ \texttt{new}(e_{tag}, e_1, e_2, \cdots) & \textrm{(data constructor)} \\
        &\ |\ e_1.e_2        & \textrm{(projection)} \\
        &\ |\ e_1.e_2 = e_3  & \textrm{(assignment)} \\
        &\ |\ \texttt{if}\ e_1\ e_2\ e_3 & \textrm{(conditional)} \\
        &\ |\ e_1(e_2)       & \textrm{(application)} \\
        &\ |\ e_1; e_2       & \textrm{(sequencing)} \\
        &\ |\ (e)          & \textrm{(grouping)} \\
        &\ |\ \texttt{function}\ x_{name}\ e_{type}\ (x_1, x_2, \cdots)\ e_{body} & \textrm{(method definition)}
\end{align*}
\end{singlespace}

Tags are a specific subset of data values generated by \texttt{new} where the
first argument is a special built-in tag \texttt{Tag}.
Though \texttt{new} is an important part of the core calculus, it is only
available in the source language in a syntactically-restricted form that
provides stronger data abstraction.

Constants are pre-built tagged values.

Types are a superset of tags that includes values generated by the
special tags \texttt{Abstract}, \texttt{Union}, and \texttt{UnionAll},
plus the special values \texttt{Any} and \texttt{Bottom}:

\vspace{-3ex}
\begin{singlespace}
\begin{align*}
  type\ ::=\ &\ \texttt{Bottom}\ |\ abstract\ |\ var \\
             &\ |\ \texttt{Union}\ type\ type \\
             &\ |\ \texttt{UnionAll}\ type\texttt{<:}var\texttt{<:}type\ type \\
             &\ |\ \texttt{Tag}\ x_{name}\ abstract_{super}\ value* \\
  abstract\ ::=\ &\ \texttt{Any}\ |\ \texttt{Abstract}\ x_{name}\ abstract_{super}\ value* \\
                 &\ |\ \texttt{Abstract}\ \texttt{Tuple}\ \texttt{Any}\ type*\ type\texttt{...}
\end{align*}
\end{singlespace}

\noindent
The last item is the special abstract varargs tuple type.

The language implicitly maps tags to data descriptions, and ensures that
the data part of a tagged value always conforms to the tag's description.
Mappings from tags to data descriptions are established by special type
declaration syntax. Data descriptions have the following grammar:

\vspace{-3ex}
\begin{singlespace}
\begin{align*}
data\ ::=\ &\ bit^n\ |\ ref\ |\ data*
\end{align*}
\end{singlespace}

\noindent
where $bit^n$ represents a string of $n$ bits, and $ref$ represents a reference
to a tagged data value. Data may be declared mutable, in which case its
representation is implicitly wrapped in a mutable cell. A built-in primitive
equality operator \texttt{===} is provided, based on $egal$ \cite{egal}
(mutable objects are compared by address, and immutable objects are compared
by directly comparing both the tag and data parts bit-for-bit, and recurring
through references to other immutable objects).

Functions are generally applied to more than one argument. In the application
syntax $e_1(e_2)$, $e_2$ is an implicitly-constructed tuple of all arguments.

We use the keyword \texttt{function} for method definitions for the sake of
familiarity, though \texttt{method} is arguably more appropriate. Method
definitions subsume lambda expressions. Each method definition modifies a
generic function named by the argument $x_{name}$. The function to extend is
specified by name rather than by value in order to make it easier to syntactically
restrict where functions can be extended. This, in turn, allows the language to
specify when new method definitions take effect, providing useful windows of
time within which methods do not change, allowing programs to be optimized more
effectively.

% (TODO describe restrictions)

The equivalent of ordinary lambda expressions can be obtained by introducing
a unique local name and defining a single method on it.
Mutable lexical bindings are provided by the usual translation to operations
on mutable cells.

The signature, or specializer, of a method is obtained by evaluating $e_{type}$,
which must result in a type value as defined above. A method has $n$
formal argument names $x_i$. The specializer must be a subtype of the
varargs tuple type of length $n$. When a method is called, its formal argument
names are bound to corresponding elements of the argument tuple. If the
specializer is a vararg type, then the last argument name is bound to a
tuple of all trailing arguments.


\section{Type system}

Our goal is to design a type system useful for describing method applicability,
and (similarly) for describing classes of values for which to specialize code.
Set-theoretic types are a natural basis for such a system.
A set-theoretic type is a symbolic expression that denotes a set of values.
In our case, these correspond to the sets of values methods are intended to apply
to, or the sets of values supported by compiler-generated method specializations.
Since set theory is widely understood, the use of such types tends to be intuitive.

These types
are less coupled to the languages they are used with, since one may design
a value domain and set relations within it without yet considering how types
relate to program terms (TODO cite Castagna). Since our goals only include
performance and expressiveness, we simply skip the later steps for now, and do
not address how to type-check terms (or, indeed, the question of whether checking
is even possible).

To avoid the dual traps of ``wasted power'' and divergence, the system we use
must have a decidable subtype relation, and must be closed under data-flow operations
(meet, join, and widen). It must also lend itself to a reasonable definition of
specificity, so that methods can be ordered automatically (a necessary property for
extensibility). These requirements are fairly strict, but still admit many possible
designs. The one we present here is aimed at providing the minimum level of
sophistication needed to yield a language that feels ``powerful'' to most modern
programmers. Beginning with the simplest possible system, we added features as
needed either to satsify the aforementioned closure properties, or to allow us to
write method definitions that seemed particularly useful (as it turns out, these
two considerations lead to essentially the same features). The presentation that
follows will partially reproduce the order of this design process.

We will define our types by formally describing their denotations as sets.
We use the notation $\llbracket T \rrbracket$ for the set denotation of
type expression $T$.
Concrete language syntax and terminal symbols of the type expression grammar
are written in typewriter font, and metasymbols are written in mathematical italic.
First there is a universal type \texttt{Any}, an empty type \texttt{Bottom}, and
a partial order $\leq$:

\vspace{-2ex}
\begin{align*}
  \llbracket \texttt{Any} \rrbracket &= \mathcal{D} \\
  \llbracket \texttt{Bottom} \rrbracket &= \emptyset \\
  T \leq S &\Leftrightarrow \llbracket T \rrbracket \subseteq \llbracket S \rrbracket
\end{align*}

\noindent
where $\mathcal{D}$ represents the domain of all values.

Next we add data objects with structured tags.
The tag of a value is accessed with \texttt{typeof(x)}.
Each tag consists of a declared type name and some number of sub-expressions,
written as \texttt{Name\{}$E_1, \cdots, E_n$\texttt{\}}.
The center dots ($\cdots$) are meta-syntactic and represent a sequence of expressions.
Tag types may have declared supertypes (written as \texttt{super(T)}).
Any type used as a supertype must be declared as abstract, meaning it
cannot have direct instances.

\vspace{-2ex}
\begin{align*}
  \llbracket \texttt{Name\{}\cdots\texttt{\}} \rrbracket &= \{ x\mid \texttt{typeof(}x\texttt{)} = \texttt{Name\{}\cdots\texttt{\}} \} \\
  \llbracket \texttt{Abstract\{}\cdots\texttt{\}} \rrbracket &= \bigcup_{\texttt{super(}T\texttt{)} = \texttt{Abstract\{}\cdots\texttt{\}}} \llbracket T \rrbracket
\end{align*}

These types closely resemble the classes of an object-oriented language with
generic (parametric) types, invariant type parameters, and no concrete inheritance.
We prefer parametric \emph{invariance} for reasons that have been addressed in the
literature \cite{Day:1995:SVC:217838.217852}.
Invariance preserves the property that the only subtypes of a concrete type are \texttt{Bottom}
and itself. We also find that most uses of covariance are more flexibly
handled by union type connectives, which will be introduced below.

Next we add conventional product (tuple) types, which are used to represent the
arguments to methods. These are almost identical to the nominal types defined above,
but are different in two ways: they are \emph{covariant} in their parameters, and permit
a special form ending in three dots (\texttt{...}) that denotes any number of trailing
elements:

\vspace{-2ex}
\begin{align*}
  \llbracket \texttt{Tuple\{}P_1,\cdots,P_n\texttt{\}} \rrbracket &= \prod_{1\leq i \leq n} \llbracket P_i \rrbracket \\
  \llbracket \texttt{Tuple\{}\cdots,P_n\texttt{...\}} \rrbracket, n\geq 1 &= \bigcup_{i\geq n-1} \llbracket \texttt{Tuple\{}\cdots,P_n^i\texttt{\}} \rrbracket
  %\llbracket \texttt{Tuple\{}\cdots\texttt{\}} \rrbracket \cup \llbracket \texttt{Tuple\{}\cdots,P_n\texttt{\}} \rrbracket \cup \llbracket \texttt{Tuple\{}\cdots,P_n,P_n\texttt{...\}} \rrbracket \\
\end{align*}

\noindent
$P_n^i$ represents $i$ repetitions of the final element $P_n$ of the type expression.

The abstract tuple types ending in \texttt{...} correspond to variadic methods, which
provide convenient interfaces for tasks like concatenating any number of arrays.
Multiple dispatch has been formulated as dispatch on tuple types before \cite{Leavens1998}.
This formulation has the advantage that \emph{any} type that is a subtype of a
tuple type can be used to express the signature of a method. It also makes the system
simpler, since subtype queries can be used to ask questions about methods.

The types introduced so far would be perfectly sufficient for many programs, and are
roughly equal in power to several multiple dispatch systems that have been designed
before. However, these types are not closed under data-flow operations. For example,
when the two branches of a conditional expression yield different types, a program
analysis must compute the union of those types to derive the type of the conditional.
The above types are not closed under set union. We therefore add the following
type connective:

\[
  \llbracket \texttt{Union\{}A,B\texttt{\}} \rrbracket = \llbracket A \rrbracket \cup \llbracket B \rrbracket \\
\]

As if by coincidence, \texttt{Union} types are also tremendously useful for expressing
method dispatch. For example, if a certain method applies to all 32-bit integers regardless
of whether they are signed or unsigned, it can be specialized for \texttt{Union\{Int32,UInt32\}}.

\texttt{Union} types are easy to understand, but complicate the type system considerably.
To see this, notice that they provide an unlimited number of ways to rewrite any type.
For example a type \texttt{T} can always be rewritten as \texttt{Union\{T,Bottom\}}, or
\texttt{Union\{Bottom,Union\{T,Bottom\}\}}, etc. Any code that processes types must
``understand'' these equivalences. \texttt{Union} types also commute with covariant
type constructors (tuples in our case), providing even more ways to rewrite types:

\[
\texttt{Tuple\{Union\{A,B\},C\}} = \texttt{Union\{Tuple\{A,C\},Tuple\{B,C\}\}}
\]

This is one of a few reasons that union types are often considered undesirable.
When used with type inference, such types can grow without bound, possibly leading
to slow or even non-terminating compilation. Their occurrence also typically
corresponds to cases that would fail most static type checkers. Yet from the
perspectives of both data-flow analysis and method specialization, they are
perfectly natural and even essential \cite{Igarashi} \cite{Smith:2008:JTI:1449764.1449804}
(TODO cite analyses that have used union types).

The next problem we need to solve arises from combining data-flow analysis
and parametric invariance. When a type constructor \texttt{C} is applied to a type
$S$ that is known only approximately at compile time, the type \texttt{C\{}$S$\texttt{\}}
does not correctly represent the result if \texttt{C} is invariant. The correct
result would be the union of all types \texttt{C\{}$T$\texttt{\}} where $T\leq S$.
Interestingly, there is again a corresponding need for such types in method
dispatch. Often one has, for example, a method that applies to arrays of any
kind of integer (\texttt{Array\{Int32\}}, \texttt{Array\{Int64\}}, etc.).
These cases can be expressed using a \texttt{UnionAll} connective, which denotes
an iterated union of a type expression for all values of a parameter in a specified
range:

\[
  \llbracket \texttt{UnionAll }L\texttt{<:T<:}U\ A \rrbracket = \bigcup_{L \leq T \leq U} \llbracket A[T/\texttt{T}] \rrbracket
\]

% TODO: The inclusion of lower bounds might make subtyping undecidable?

This is equivalent to an existential type \cite{boundedquant};
for each concrete subtype of it there exists a corresponding $T$.
Anecdotally, programmers often find existential types confusing.
We prefer the union interpretation because we are describing sets of values;
the notion of ``there exists'' can be semantically misleading since it sounds like
only a single $T$ value might be involved.

%Conjecture: these types are intuitive to dispatch on because they correspond
%to program behavior in the same way that dataflow analysis approximates program
%behavior.

% $T=S \longleftrightarrow (T\leq S) \wedge (S\leq T)$.

\subsection{Examples}

\texttt{UnionAll} types are quite expressive. In combination with nominal
types they can describe groups of containers such as
\texttt{UnionAll T<:Number Array\{Array\{T\}\}} (all arrays of arrays of
some kind of number) or
\texttt{Array\{UnionAll T<:Number Array\{T\}\}} (an array of arrays of
potentially different types of number).

In combination with tuple types, \texttt{UnionAll} types provide powerful
method dispatch specifications. For example
\texttt{UnionAll T Tuple\{Array\{T\},Int,T\}} matches three arguments:
an array, an integer, and a value that is an instance of the array's
element type. This is a natural signature for a method that assigns a
value to a given index within an array.


\subsection{Type constructors}

It is important for any proposed high-level technical computing language to be
simple and approachable, since otherwise the value over established
powerful-but-complex languages like C++ is less clear.
In particular, type parameters raise usability concerns.
Needing to write parameters along with every type is verbose, and requires users
to know more about the type system and to know more details of particular
types (how many parameters they have and what each one means).
Furthermore, in many contexts type parameters are not directly relevant.
For example, a large amount of code operates on \texttt{Array}s of any
element type, and in these cases it should be possible to ignore type parameters.

Consider \texttt{Array\{T\}}, the type of arrays with element type \texttt{T}.
In most languages with parametric types, the identifier \texttt{Array} would
refer to a type constructor, i.e. a type of a different \emph{kind} than
ordinary types like \texttt{Int} or \texttt{Array\{Int\}}.
Instead, we find it intuitive and appealing for \texttt{Array} to refer to
any kind of array, so that a declaration such as \texttt{x::Array} simply
asserts \texttt{x} to be some kind of array. In other words,

\[
\texttt{Array} = \texttt{UnionAll T Array$^\prime$\{T\}}
\]

\noindent
where \texttt{Array$^\prime$} refers to a hidden, internal type constructor.
The \texttt{\{ \}} syntax can then be used to instantiate a \texttt{UnionAll}
type at a particular parameter value.

\subsection{Associated types and type computation}

\subsection{Subtyping}

Describe algorithm.

Very likely $\Pi_2^{\textrm{P}}$-hard.
Checking a subtype relation with unions requires checking that for all choices
on the left, there exists a choice on the right that makes the relation hold.
This matches the quantifier structure of 2-TQBF problems of the form
$\forall x_i . \exists y_i . F$ where $F$ is a logical formula. If the formula
is rewritten in conjunctive normal form, it corresponds to subtype checking
between two tuple types, where the relation must hold for each pair of
corresponding types. Now use a type \texttt{N\{}$x$\texttt{\}} to
represent $\neg x$. The clause $(x_i \vee y_i)$ can be translated to
$x_i$\texttt{ <: Union\{N\{}$y_i$\texttt{\}, True\}} (where the $x_i$ and
$y_i$ are type variables bound by \texttt{UnionAll} on the left and right,
respectively).


\subsection{Type system variants}

features that are fairly straightforward to add:

\vspace{-2ex}
\begin{singlespace}
\begin{itemize}
\item structurally-subtyped records
\item mu-recursive types (regular trees)
\item regular types (allowing ... in more places)
\end{itemize}
\end{singlespace}

\noindent
features that are difficult to add, or possibly break decidability:

\vspace{-2ex}
\begin{singlespace}
\begin{itemize}
\item arrow types
\item negations
\item intersections, multiple inheritance
\item universal quantifiers
\item arbitrary predicates, theory of natural numbers, etc.
\end{itemize}
\end{singlespace}

\section{Dispatch mechanism}

\subsection{Ambiguities}

\section{Higher-order programming}

Generic functions are first-class objects, and so can be passed as arguments
just as in any dynamically-typed language with first-class functions.
However, assigning useful type tags to generic functions and deciding how
they should dispatch is not so simple. Past work has often described the
types of generic functions using the ``intersection of arrows'' formalism
\cite{RonchiDellaRocca:1988:PTS:55079.55086} \cite{Dunfield:2012:EIU:2364527.2364534}
\cite{boundedquant} \cite{Castagna:1995:COF:203496.203510}. Since an ordinary
function has an arrow type $A\rightarrow B$ describing how it maps arguments
$A$ to results $B$, a function with multiple definitions can natually be
considered to have multiple such types. For example, a \texttt{sin} function
with the following two definitions:

\begin{singlespace}
\begin{lstlisting}[style=customjulia]
sin(x::Float64) = # compute sine of x in double precision
sin(v::Vector) = map(sin, v)
\end{lstlisting}
\end{singlespace}

\noindent
could have the type $(\texttt{Float64}\rightarrow\texttt{Float64})\cap(\texttt{Vector}\rightarrow\texttt{Vector})$. The intuition is that this \texttt{sin} function can be
used both where a $\texttt{Float64}\rightarrow\texttt{Float64}$ function
is expected and where a $\texttt{Vector}\rightarrow\texttt{Vector}$ function is expected,
and therefore its type is the intersection of these types.

This approach is effective for statically checking uses of generic
functions: anywhere a function goes, we must keep track of which arrow
types it ``contains'' in order to be sure that at least one matches
every call site and allows the surrounding code to type check.
However, despite the naturalness of this typing of generic functions,
this formulation is quite problematic for dispatch and code specialization
(not to mention that it might make subtyping undecidable).

\subsection{Problems for code selection}

Consider what happens when we try to define an integration function:

\begin{singlespace}
\begin{lstlisting}[style=customjulia]
# 1-d integration of a real-valued function
integrate(f::Float64->Float64, x0, x1)

# multi-dimensional integration of a vector-valued function
integrate(f::Vector->Vector, v0, v1)
\end{lstlisting}
\end{singlespace}

\noindent
The \texttt{->} is not real Julia syntax, but is assumed for the sake of
this example.
Here we wish to select a different integration routine based on what
kind of function is to be integrated.
However, these definitions are ambiguous with respect to the \texttt{sin}
function defined above.
Of course, the potential for method ambiguities existed already.
However this sort of ambiguity is introduced \emph{non-locally} ---
it cannot be detected when the \texttt{integrate} methods are defined.

%re: selection:
%- feels like the wrong abstraction. GFs are the means of selecting behavior,
%- changes over time
%- might depend on inference results

Such a non-local introduction of ambiguity is a special case of the
general problem that a generic function's type would change depending
on what definitions have been added (which depends e.g. on which libraries
have been loaded). This does not feel like the right abstraction:
type tags are supposed to form a ``ground truth'' about objects against
which program behavior can be selected. Though generic functions change
with the addition of methods, it would be more satisfying for their
types to somehow reflect an intrinsic, unchanging property.

A slightly different approach we might try would be to immitate
the types of higher-order functions in traditional
statically-typed functional languages. For example, we might wish
to write \texttt{map} as follows:

\begin{singlespace}
\begin{lstlisting}[style=customjulia]
map{A,B}(f::A->B, x::List{A}) =
  isempty(x) ? List{B}() : List{B}(f(head(x)), map(f, tail(x)))
\end{lstlisting}
\end{singlespace}

The idea is for the first argument to match any function, and not use
the arrow type for dispatch, thereby avoiding ambiguity problems.
Instead, immediately after method selection, values for \texttt{A} and
\texttt{B} would be determined using the element type of \texttt{x}
and the table of definitions of \texttt{f}.

Unfortunately it is not clear how exactly \texttt{B} should be
determined. We could require return type declarations on every method,
but this would adversely affect usability (such declarations would also
be helpful if we wanted to dispatch on arrow types, though they would
not solve the ambiguity problem). Or we could use type inference
of \texttt{f} on argument type \texttt{A}. This would not work very
well, since the result would depend on partly-arbitrary heuristics.
Such heuristics are fine for merely \emph{analyzing} a program, but
are not appropriate for determining the value of a user-visible
variable, as this would make program behavior unpredictable.


\subsection{Problems for code specialization}

re: specialization:

- GFs have many methods and no two are the same in practice

- there is no way to restrict a function to one definition

if we have f(g::String->String) and call it with

x::Int->Int
f(x)

where x has only one definition, we would expect the method not to be
applicable. however it is applicable with type $Int->Int \cap String->String$

specialization wants to exclude irrelevant cases, and intersections
*manufacture* irrelevant cases!

\subsection{Possible solutions}
solutions:

identity-typing

nominal function types

arrow types without intersections, used to ``slice'' generic functions

(generic functions with declared overall types)

\subsection{Implementing \texttt{map}}

\section{Performance model}

\subsection{Type inference}

\subsection{Specialization}
