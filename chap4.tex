\chapter{The Julia approach}

\section{Language description}

\subsection{Type system}

Our goal is to design a type system useful for describing method applicability,
and (similarly) for describing classes of values for which to specialize code.
Set-theoretic types are a natural basis for such a system.
A set-theoretic type is a symbolic expression that denotes a set of values.
In our case, these correspond to the sets of values methods are intended to apply
to, or the sets of values supported by compiler-generated method specializations.
Since set theory is widely understood, the use of such types tends to be intuitive.

These types
are less coupled to the languages they are used with, since one may design
a value domain and set relations within it without yet considering how types
relate to program terms (TODO cite Castagna). Since our goals only include
performance and expressiveness, we simply skip the later steps for now, and do
not address how to type-check terms (or, indeed, the question of whether checking
is even possible).

To avoid the dual traps of ``wasted power'' and divergence, the system we use
must have a decidable subtype relation, and must be closed under data-flow operations
(meet, join, and widen). It must also lend itself to a reasonable definition of
specificity, so that methods can be ordered automatically (a necessary property for
extensibility). These requirements are fairly strict, but still admit many possible
designs. The one we present here is aimed at providing the minimum level of
sophistication needed to yield a language that feels ``powerful'' to most modern
programmers. Beginning with the simplest possible system, we added features as
needed either to satsify the aforementioned closure properties, or to allow us to
write method definitions that seemed particularly useful (as it turns out, these
two considerations lead to essentially the same features). The presentation that
follows will partially reproduce the order of this design process.

We will define our types by formally describing their denotations as sets.
We use the notation $\llbracket T \rrbracket$ for the set denotation of
type expression $T$.
Concrete language syntax and terminal symbols of the type expression grammar
are written in typewriter font, and metasymbols are written in mathematical italic.
First there is a universal type \texttt{Any}, an empty type \texttt{Bottom}, and
a partial order $\leq$:

\vspace{-2ex}
\begin{align*}
  \llbracket \texttt{Any} \rrbracket &= \mathcal{D} \\
  \llbracket \texttt{Bottom} \rrbracket &= \emptyset \\
  T \leq S &\Leftrightarrow \llbracket T \rrbracket \subseteq \llbracket S \rrbracket
\end{align*}

\noindent
where $\mathcal{D}$ represents the domain of all values.

Next we add data objects with structured tags.
The tag of a value is accessed with \texttt{typeof(x)}.
Each tag consists of a declared type name and some number of sub-expressions,
written as \texttt{Name\{}$E_1, \cdots, E_n$\texttt{\}}.
The center dots ($\cdots$) are meta-syntactic and represent a sequence of expressions.
Tag types may have declared supertypes (written as \texttt{super(T)}).
Any type used as a supertype must be declared as abstract, meaning it
cannot have direct instances.

\vspace{-2ex}
\begin{align*}
  \llbracket \texttt{Name\{}\cdots\texttt{\}} \rrbracket &= \{ x\mid \texttt{typeof(}x\texttt{)} = \texttt{Name\{}\cdots\texttt{\}} \} \\
  \llbracket \texttt{Abstract\{}\cdots\texttt{\}} \rrbracket &= \bigcup_{\texttt{super(}T\texttt{)} = \texttt{Abstract\{}\cdots\texttt{\}}} \llbracket T \rrbracket
\end{align*}

These types closely resemble the classes of an object-oriented language with
generic (parametric) types, invariant type parameters, and no concrete inheritance.
We prefer parametric \emph{invariance} for reasons that have been addressed in the
literature \cite{Day:1995:SVC:217838.217852}.
Invariance preserves the property that the only subtypes of a concrete type are \texttt{Bottom}
and itself. We also find that most uses of covariance are more flexibly
handled by union type connectives, which will be introduced below.

Next we add conventional product (tuple) types, which are used to represent the
arguments to methods. These are almost identical to the nominal types defined above,
but are different in two ways: they are \emph{covariant} in their parameters, and permit
a special form ending in three dots (\texttt{...}) that denotes any number of trailing
elements:

\vspace{-2ex}
\begin{align*}
  \llbracket \texttt{Tuple\{}P_1,\cdots,P_n\texttt{\}} \rrbracket &= \prod_{1\leq i \leq n} \llbracket P_i \rrbracket \\
  \llbracket \texttt{Tuple\{}\cdots,P_n\texttt{...\}} \rrbracket, n\geq 1 &= \bigcup_{i\geq n-1} \llbracket \texttt{Tuple\{}\cdots,P_n^i\texttt{\}} \rrbracket
  %\llbracket \texttt{Tuple\{}\cdots\texttt{\}} \rrbracket \cup \llbracket \texttt{Tuple\{}\cdots,P_n\texttt{\}} \rrbracket \cup \llbracket \texttt{Tuple\{}\cdots,P_n,P_n\texttt{...\}} \rrbracket \\
\end{align*}

\noindent
$P_n^i$ represents $i$ repetitions of the final element $P_n$ of the type expression.

The abstract tuple types ending in \texttt{...} correspond to variadic methods, which
provide convenient interfaces for tasks like concatenating any number of arrays.
Multiple dispatch has been formulated as dispatch on tuple types before \cite{Leavens1998}.
This formulation has the advantage that \emph{any} type that is a subtype of a
tuple type can be used to express the signature of a method. It also makes the system
simpler, since subtype queries can be used to ask questions about methods.

The types introduced so far would be perfectly sufficient for many programs, and are
roughly equal in power to several multiple dispatch systems that have been designed
before. However, these types are not closed under data-flow operations. For example,
when the two branches of a conditional expression yield different types, a program
analysis must compute the union of those types to derive the type of the conditional.
The above types are not closed under set union. We therefore add the following
type connective:

\[
  \llbracket \texttt{Union\{}A,B\texttt{\}} \rrbracket = \llbracket A \rrbracket \cup \llbracket B \rrbracket \\
\]

As if by coincidence, \texttt{Union} types are also tremendously useful for expressing
method dispatch. For example, if a certain method applies to all 32-bit integers regardless
of whether they are signed or unsigned, it can be specialized for \texttt{Union\{Int32,UInt32\}}.

\texttt{Union} types are easy to understand, but complicate the type system considerably.
To see this, notice that they provide an unlimited number of ways to rewrite any type.
For example a type \texttt{T} can always be rewritten as \texttt{Union\{T,Bottom\}}, or
\texttt{Union\{Bottom,Union\{T,Bottom\}\}}, etc. Any code that processes types must
``understand'' these equivalences. \texttt{Union} types also commute with covariant
type constructors (tuples in our case), providing even more ways to rewrite types:

\[
\texttt{Tuple\{Union\{A,B\},C\}} = \texttt{Union\{Tuple\{A,C\},Tuple\{B,C\}\}}
\]

This is one of a few reasons that union types are often considered undesirable.
When used with type inference, such types can grow without bound, possibly leading
to slow or even non-terminating compilation. Their occurrence also typically
corresponds to cases that would fail most static type checkers. Yet from the
perspectives of both data-flow analysis and method specialization, they are
perfectly natural and even essential \cite{Igarashi} \cite{Smith:2008:JTI:1449764.1449804}
(TODO cite analyses that have used union types).

The next problem we need to solve arises from combining data-flow analysis
and parametric invariance. When a type constructor \texttt{C} is applied to a type
$S$ that is known only approximately at compile time, the type \texttt{C\{}$S$\texttt{\}}
does not correctly represent the result if \texttt{C} is invariant. The correct
result would be the union of all types \texttt{C\{}$T$\texttt{\}} where $T\leq S$.
Interestingly, there is again a corresponding need for such types in method
dispatch. Often one has, for example, a method that applies to arrays of any
kind of integer (\texttt{Array\{Int32\}}, \texttt{Array\{Int64\}}, etc.).
These cases can be expressed using a \texttt{UnionAll} connective, which denotes
an iterated union of a type expression for all values of a parameter in a specified
range:

\[
  \llbracket \texttt{UnionAll }L\texttt{<:T<:}U\ A \rrbracket = \bigcup_{L \leq T \leq U} \llbracket A[T/\texttt{T}] \rrbracket
\]

% TODO: The inclusion of lower bounds probably makes subtyping undecidable,
% since it can encode contravariance. Needs to be removed.

This is equivalent to an existential type \cite{boundedquant};
for each concrete subtype of it there exists a corresponding $T$.
Anecdotally, programmers have often found existential types confussing.
We prefer the union interpretation because we are describing sets of values;
the notion of ``there exists'' can be semantically misleading since it sounds like
only a single $T$ value might be involved.

%Conjecture: these types are intuitive to dispatch on because they correspond
%to program behavior in the same way that dataflow analysis approximates program
%behavior.

% $T=S \longleftrightarrow (T\leq S) \wedge (S\leq T)$.

\subsubsection{Examples}

\texttt{UnionAll} types are quite expressive. In combination with nominal
types they can describe groups of containers such as
\texttt{UnionAll T<:Number Array\{Array\{T\}\}} (all arrays of arrays of
some kind of number) or
\texttt{Array\{UnionAll T<:Number Array\{T\}\}} (an array of arrays of
potentially different types of number).

In combination with tuple types, \texttt{UnionAll} types provide powerful
method dispatch specifications. For example
\texttt{UnionAll T Tuple\{Array\{T\},Int,T\}} matches three arguments:
an array, an integer, and a value that is an instance of the array's
element type. This is a natural signature for a method that assigns a
value to a given index within an array.


\subsubsection{Type constructors}

It is important for any proposed high-level technical computing language to be
simple and approachable, since otherwise the value over established
powerful-but-complex languages like C++ is less clear.
In particular, type parameters raise usability concerns.
Needing to write parameters along with every type is verbose, and requires users
to know more about the type system and to know more details of particular
types (how many parameters they have and what each one means).
Furthermore, in many contexts type parameters are not directly relevant.
For example, a large amount of code operates on \texttt{Array}s of any
element type, and in these cases it should be possible to ignore type parameters.

Consider \texttt{Array\{T\}}, the type of arrays with element type \texttt{T}.
In most languages with parametric types, the identifier \texttt{Array} would
refer to a type constructor, i.e. a type of a different \emph{kind} than
ordinary types like \texttt{Int} or \texttt{Array\{Int\}}.
Instead, we find it intuitive and appealing for \texttt{Array} to refer to
any kind of array, so that a declaration such as \texttt{x::Array} simply
asserts \texttt{x} to be some kind of array. In other words,

\[
\texttt{Array} = \texttt{UnionAll T Array$^\prime$\{T\}}
\]

\noindent
where \texttt{Array$^\prime$} refers to a hidden, internal type constructor.
The \texttt{\{ \}} syntax can then be used to instantiate a \texttt{UnionAll}
type at a particular parameter value.

\subsubsection{Subtyping}

Describe algorithm.

Very likely $\Pi_2^{\textrm{P}}$-hard.
Checking a subtype relation with unions requires checking that for all choices
on the left, there exists a choice on the right that makes the relation hold.
This matches the quantifier structure of 2-TQBF problems of the form
$\forall x_i . \exists y_i . F$ where $F$ is a logical formula. If the formula
is rewritten in conjunctive normal form, it corresponds to subtype checking
between two tuple types, where the relation must hold for each pair of
corresponding types. Now use a type \texttt{N\{}$x$\texttt{\}} to
represent $\neg x$. The clause $(x_i \vee y_i)$ can be translated to
$x_i$\texttt{ <: Union\{N\{}$y_i$\texttt{\}, True\}} (where the $x_i$ and
$y_i$ are type variables bound by \texttt{UnionAll} on the left and right,
respectively).


\subsubsection{Type system variants}

features that are fairly straightforward to add:

\vspace{-2ex}
\begin{singlespace}
\begin{itemize}
\item structurally-subtyped records
\item mu-recursive types (regular trees)
\item regular types (allowing ... in more places)
\end{itemize}
\end{singlespace}

\noindent
features that are difficult to add, or possibly break decidability:

\vspace{-2ex}
\begin{singlespace}
\begin{itemize}
\item arrow types
\item negations
\item intersections, multiple inheritance
\item universal quantifiers
\item lower bounds in quantifiers
\item arbitrary predicates, theory of natural numbers, etc.
\end{itemize}
\end{singlespace}

\subsection{Data model}

\subsection{Dispatch mechanism}

\section{Performance model}

\subsection{Type inference}

\subsection{Specialization}

\section{Explication through elimination}

\subsection{Conversion and other basic operations}

This section will illustrate how we implement key features of technical computing
systems using our methodology.

- The abstractions of equality and comparison. Different equivalence classes between

is/===, isequal and ==

- Numeric vs lexicographic ordering?

cmp, lexcmp, vs isless, <

\subsection{Numeric types and embeddings}

We might prefer ``number'' to be a single,
concrete concept, but the history of mathematics has seen the concept
extended many times, from integers to rationals to reals, and then to complex,
quaternion, and more. These constructions tend to follow a pattern: a new set
of numbers is constructed around a subset isomorphic to an existing set of
numbers. For example, the reals are isomorphic to the complex numbers with
zero imaginary part.

Human beings happen to be good at equating and moving between isomorphic sets,
so it is easy to imagine that the reals and complexes with zero imaginary
part are one and the same. But a computer forces us to be specific, and admit
that a real number is not complex, and a complex number is not real. And yet
the close relationship between them is too compelling not to model in a
computer somehow. Here we have a numerical analog to the famous ``circle and
ellipse'' problem in object-oriented programming: the set of circles is
isomorphic to the set of ellipses with equal axes, yet neither ``is a''
relationship in a class hierarchy seems fully correct. An ellipse is not
a circle, and in general a circle cannot serve as an ellipse (for example,
the set of circles is not closed under the same operations that the set of
ellipses is, so a program written for ellipses might not work on circles).
This problem implies that a single built-in type hierarchy is not
sufficient: we want to model custom *kinds* of relationships between
types (e.g. ``can be embedded in'' in addition to ``is a'').

Two further problems should also be kept in mind. First, the natural isomorphisms
between sets of numbers might not be isomorphisms on a real computer. For example,
due to the behavior of floating-point arithmetic, an operation on complex numbers
with zero imaginary part might not give an answer equal to the same operation on
real numbers. Second, the contexts that demand use of one type of number or
another are often not easily described by type systems. The classic example is
square root (\texttt{sqrt}), whose result is complex for negative arguments.
Including a number's sign in its type is a possibility, but this quickly gets
out of hand --- should a type system attempt to prove a matrix symmetric before
we compute its eigenvalues? While we cannot offer a once-and-for-all solution
to these problems, we will show how the flexibility of our proposed mechanism
is useful for addressing them.

% ex: generic sum function accumulating result in at least
% double precision. just using +::T->T->T doesn't work.

\subsubsection{Implementing type embeddings}

Most functions are naturally implemented in the value domain, but some are
actually easier to implement in the type domain. One reason is that there
is a bottom element, which most data types lack.

\subsubsection{Diversity of number and number-like types in practice}

Originally, our reasons for implementing all numeric types at the library
level were not entirely practical. We had a principled opposition to
including such definitions in a compiler, and guessed that being able to
define numeric types would help ensure the language was ``powerful enough''.
However, defining numeric and number-like types and their interactions turns
out to be surprisingly useful. Once such types become easy to obtain,
people find more and more uses for them.

Ordinal types: Pointer, Char

Integer types: Int8, Int16, Int32, Int64, Int128, UInt8, UInt16, UInt32, UInt64, UInt128, BigInt

Floating point types: Float16, Float32, Float64, BigFloat
(Float128, double-double)

Fixed point: Fixed32{b}, Ufixed8, Ufixed16

Extensions: Complex, Quaternion, Interval, DualNumber

Number-like: Date, TimePeriod, DatePeriod, Color, (Units), DNA nucleotide type (bioseq.jl)
dates: different cardinal and ordinal behavior

musical notes

\subsubsection{Applications}

\emph{Ranges} illustrate an interesting application of type promotion.
A range data type, notated \texttt{a:s:b}, represents a sequence of values
starting at \texttt{a} and ending at \texttt{b}, with a distance of \texttt{s}
between elements (internally, this notation is translated to
\texttt{colon(a, s, b)}). Ranges seem simple enough, but a reliable,
efficient, and generic implementation is difficult to achieve.
We propose the following requirements:

\begin{itemize}
\item The start and stop values can be passed as different types, but internally
  should be of the same type.
\item Ranges should work with ordinal types, not just numbers (examples include
  characters, pointers, and calendar dates).
\item If any of the arguments is a floating-point number, a special
  \texttt{FloatRange} type designed to cope well with roundoff is returned.
\end{itemize}

In the case of ordinal types, the step value is naturally of a different type
than the elements of the range. For example, one may add 1 to a character to
get the ``next'' encoded character, but it does not make sense to add two
characters.

It turns out that the desired behavior can be achieved with six definitions:

First, given three floats of the same type we can construct a \texttt{FloatRange}
right away:

\begin{verbatim}
colon{T<:FloatingPoint}(start::T, step::T, stop::T) = FloatRange{T}(start, step, stop)
\end{verbatim}

Next, if \texttt{a} and \texttt{b} are of the same type and there are no floats,
we can construct a general range:

\begin{verbatim}
colon{T}(start::T, step, stop::T) = StepRange(start, step, stop)
\end{verbatim}

Now there is a problem to fix: if the first and last arguments are of some
non-floating-point numeric type, but the step is floating point, we want to
promote all arguments to a common floating point type. We must also do this
if the first and last arguments are floats, but the step is some other kind
of number:

\begin{verbatim}
colon{T<:Real}(a::T, s::FloatingPoint, b::T) = colon(promote(a,s,b)...)

colon{T<:FloatingPoint}(a::T, s::Real, b::T) = colon(promote(a,s,b)...)
\end{verbatim}

These two definitions are correct, but ambiguous: if the step is a float
of a different type than \texttt{a} and \texttt{b} both definitions are
equally applicable. We can add the following disambiguating definition:

\begin{verbatim}
colon{T<:FloatingPoint}(a::T, s::FloatingPoint, b::T) = colon(promote(a,s,b)...)
\end{verbatim}

All of these five definitions require \texttt{a} and \texttt{b} to be of the
same type. If they are not, we must promote just those two arguments, and leave
the step alone in case we are dealing with ordinal types:

\begin{verbatim}
colon{A,B}(a::A, s, b::B) = colon(convert(promote_type(A,B),a), s, convert(promote_type(A,B),b))
\end{verbatim}

This example shows that it is not always sufficient to have a built-in set of
``promoting operators''. Library functions like this \texttt{colon} need more
control.


\subsubsection{Current approaches}

Numbers tend to be among the most
complex features of a language. Numeric types usually need to be a special
case: in a typical language with built-in numeric types, describing their
behavior is beyond the expressive power of the language itself. For example,
in C arithmetic operators like \texttt{+} accept multiple types of arguments
(ints and floats), but no user-defined C function can do this (this situation
is of course improved in C++). In Python, a special arrangement is made for
\texttt{+} to call either an \texttt{\_\_add\_\_} or \texttt{\_\_radd\_\_} method,
effectively providing double-dispatch for arithmetic in a language that is
idiomatically single-dispatch.



\subsection{Multidimensional array indexing}

One-dimensional arrays are a simple and essential data structure found in
most programming languages. The multi-dimensional arrays required in
scientific computing, however, are a different beast entirely. Allowing
any number of dimensions entails a significant increase in complexity. Why?
The essential reason is that core properties of the data structure no
longer fit in a constant amount of space. The space needed to store the
sizes of the dimensions (the array shape) is proportional to the number
of dimensions. This does not seem so bad, but becomes a large problem
due to three additional facts.

% TODO: break up using enumerate
First, code that operates on the dimension
sizes needs to be highly efficient. Typically the overhead of a loop is
unacceptable, and such code needs to be fully unrolled. Second, in some
code the number of dimensions is a \emph{dynamic} property --- it is
only known at run time. Third, programs may wish to treat arrays with
different numbers of dimensions very differently. A vector (1d) might
have rather different behaviors than a matrix (2d) (for example, to
compute a norm). This kind of
behavior makes the number of dimensions a crucial part of program
semantics, preventing it from remaining a compiler implementation detail.

% TODO: break up using enumerate
These facts pull in different directions. The first fact asks for static
analysis. The second fact asks for run-time flexibility. The third fact asks
for dimensionality to be part of the type system, but partly determined
at run time (for example, via virtual method dispatch). Current approaches
choose a compromise. In some systems, the number of dimensions has a strict
limit (e.g. 3 or 4), so that separate classes for each case may be written
out in full. Other systems choose flexibility, and just accept that most
or all operations will be dynamically dispatched. Other systems might
provide flexibility only at compile time, for example a template library
where the number of dimensions must be statically known.

%% TODO: insert example of limited power of C++ array libraries

%% TODO: examples of systems limited to n==3

Whatever decision is made, rules must be defined for how various operators
act on dimensions. For now we will focus on indexing, since selecting
parts of arrays has particularly rich behavior with respect to
dimensionality. For example, if a single row or column of a matrix is
selected, does the result have one or two dimensions? Array implementations
prefer to invoke general rules to answer such questions. Such a rule might
say ``dimensions indexed with scalars are dropped'', or ``trailing
dimensions of size one are dropped'', or ``the rank of the result
is the sum of the ranks of the indexes'' (as in APL).

%%%
Our goal here is a bit unusual: we are not concerned with which rules
might work best, but merely with how they can be specified, so that
domain experts can experiment.

In fact different domains want different things. E.g. in images, each
dimension might be quite different, e.g. time vs. space vs. color,
so you don't want to drop or rearrange dimensions very often.
%%%

Here are our ground rules:

\begin{enumerate}
\item You can't manually implement the behavior inside the compiler
\item The compiler must be able to reasonably understand the program
\item The code must be reasonably easy to write
\end{enumerate}


How are such rules implemented? For a
language with built-in multidimensional arrays, the compiler will
analyze indexing expressions and determine an answer using hard-coded
logic.
% TODO: grab example from one of these compilers
However, this approach is not satisfying: we would rather
implement the behavior in libraries, so that different kinds of arrays
may be defined, or so that rules of similar complexity may be
defined for other kinds of objects. But these kinds of rules are
unusually difficult to implement in libraries. If a library writes out
its indexing logic using imperative code, the host language compiler
is not likely to be able to analyze it. Using compile-time abstracton
(templates) would provide better performance, but such libraries tend
to be difficult to write (and read), and the full complement of
indexing behavior expected by technical users strains the capabilities
of such systems.

% TODO: insert actual example of numpy

Our dispatch mechanism permits a novel solution. If a multiple dispatch
system supports variadic functions and argument ``splicing'' (the ability
to pass a structure of $n$ values as $n$ separate arguments to a function),
then indexing behavior can be defined as method signatures.

This solution is still a compromise among the factors outlined above,
but it is a new compromise that provides a net-better solution.
% TODO more

Below we define a function \texttt{index\_shape} that computes the
shape of a result array given a series of index arguments. We show
three versions, each implementing a different rule that users in
different domains might want:

% TODO: point out array = (shape, data), so those are the two parts
% we need to handle. In julia the ``data'' part is not a first class
% object; it is not directly exposed to the user, but this is more
% of an implementation detail.

\begin{verbatim}
# drop dimensions indexed with scalars
index_shape() = ()
index_shape(i::Real, I...) = index_shape(I...)
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}

\begin{verbatim}
# drop trailing dimensions indexed with scalars
index_shape(i::Real...) = ()
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}

\begin{verbatim}
# rank summing (APL)
index_shape() = ()
index_shape(i, I...) = tuple(size(i)..., index_shape(I...)...)
\end{verbatim}

Inferring the length of the result of \texttt{index\_shape} is sufficient
to infer the rank of the result array.

These definitions are concise, easy to write, and possible for a
compiler to understand fully using straightforward techniques.

% TODO: point out how this combines the ``object part'' and the
% ``array part'' into a coherent whole.

Here is a sample derivation for the call \texttt{index\_shape(1:m,1,1:n)}
(the argument type tuple is \texttt{(Range1,Int,Range1)}), using the first
definition above (dropping scalar-indexed dimensions):

\begin{verbatim}
index_shape(1:n, 1, 1:m) => tuple(length(::Range1)::Int, index_shape((::Int, ::Range1)...)...)

index_shape((::Int, ::Range1)...) => index_shape(::Int, ::Range1)

index_shape(::Int, ::Range1) => index_shape((::Range1,)...)

index_shape((::Range1,)...) => index_shape(::Range1)

index_shape(::Range1) => tuple(length(::Range1)::Int, index_shape(()...)...)

index_shape(()...) => index_shape()::()

back substitute => tuple(length(::Range1)::Int, index_shape(()...)::()...)::(Int,)

back substitute => tuple(length(::Range1)::Int, ::(Int,)...)

tuple(::Int, ::(Int,)...) => tuple(::Int, ::Int)

::(Int, Int)

\end{verbatim}

The result type is determined using only dataflow type inference, plus a
rule for splicing an immediate container (the type of \texttt{f((a,b)...)} is
the type of \texttt{f(a,b)}). Argument list destructuring takes place inside
the type intersection operator used to combine argument types with method
signatures.

This approach does not depend on any heuristics. Each call to \texttt{index\_shape}
simply requires one recursive invocation of type inference. This process reaches
the base case \texttt{()} for these definitions, since each recursive call
handles a shorter argument list (for less-well-behaved definitions, we might
end up invoking a widening operator instead).


\begin{verbatim}
diverge() = randbool() ? () : tuple(1, diverge()...)
\end{verbatim}

\subsection{Array views}

\subsection{Units}

\subsection{Even more elimination?}

Some features of the language could be even further eliminated. For example data
types could be implemented in terms of lambda abstractions. But certain patterns
are so useful that they might as well be provided in a standard form. It also
probably makes the compiler much more efficient not to need to pass around and
repeatedly analyze full representations of the meanings of such ubiquitous constructs.
